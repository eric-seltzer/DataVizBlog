[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog was created for my Data Visualization class. My goal is to create interpretable visualizations that are easy for non statistics people to understand, as well as having hints to further interpretations if you look further. I wanted to combine this with things I am passionate about. This is why there is a blog post about stock prices, soccer, and basketball. I hope to expand on these subjects well also improving the visualizatoin skills and my analytical skills. I hope you enjoy!"
  },
  {
    "objectID": "posts/blog-post-3/blog-post-3.html",
    "href": "posts/blog-post-3/blog-post-3.html",
    "title": "Predicting if a NBA Team Will Make the Playoffs",
    "section": "",
    "text": "Intoduction\nFor this blog post, I wanted to take on more of a challenging idea which was to both create a prediction model and visualize that data. To to this, I also wanted to combine this with my passion for sports, particularly basketball. It felt only right to use NBA data to accomplish this. On Data World, I found a data set that had team stats from the 97-98 season all the way until the 21-22 season.\n\n# loading in necessary packages\nlibrary(tidyverse)\nlibrary(here)\nlibrary(kableExtra)\nlibrary(broom)\nlibrary(modelr)\nlibrary(knitr)\n\n\n# loading in data set\nteam_stats &lt;- read_csv(here('data/NBA_Team_Stats.csv'))\n\nHere is an example of what the data set looks like directly from the source.\n\n\nOriginal Data Set\n\nkable(head(team_stats))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo\nTeam\nG\nMin\nPts\nReb\nAst\nStl\nBlk\nTo\nPf\nDreb\nOreb\nFgm-a\nPct…15\n3gm-a\nPct…17\nFtm-a\nPct…19\nEff\nDeff\nYear\n\n\n\n\n1\nChicago\n103\n48.4\n96.0\n44.1\n23.1\n8.6\n4.3\n13.0\n21.1\n29.2\n14.9\n36.7-81.7\n0.449\n3.9-12.0\n0.323\n18.7-25.2\n0.741\n111.6\n17.5\n1997-1998\n\n\n2\nUtah\n102\n48.3\n98.6\n40.8\n24.7\n7.6\n4.8\n14.7\n24.3\n29.5\n11.3\n35.9-74.3\n0.483\n3.1-8.4\n0.368\n23.8-30.9\n0.768\n116.3\n17.5\n1997-1998\n\n\n3\nPhoenix\n86\n48.6\n99.3\n41.9\n25.6\n9.2\n5.3\n14.4\n21.7\n29.8\n12.1\n38.2-82.0\n0.466\n5.2-14.7\n0.355\n17.7-23.6\n0.747\n117.1\n13.6\n1997-1998\n\n\n4\nL.A.Lakers\n95\n48.3\n104.8\n42.9\n24.3\n8.7\n6.8\n14.7\n22.9\n29.7\n13.2\n38.0-79.1\n0.480\n6.1-17.3\n0.350\n22.8-33.7\n0.675\n120.8\n13.2\n1997-1998\n\n\n5\nSan Antonio\n91\n48.4\n92.5\n44.1\n21.9\n6.2\n6.9\n15.3\n21.2\n32.2\n11.9\n35.1-75.1\n0.468\n3.7-10.8\n0.344\n18.5-26.8\n0.688\n108.0\n13.1\n1997-1998\n\n\n6\nIndiana\n98\n48.4\n95.3\n38.6\n22.4\n7.8\n4.5\n13.6\n23.0\n28.2\n10.4\n35.0-74.9\n0.468\n5.0-12.9\n0.387\n20.3-26.6\n0.763\n109.0\n12.2\n1997-1998\n\n\n\n\n\nMy first step in this project is to clean and format this data in a way so that I can work with it how I want too. Below is a table showing what all the variable names mean.\n\n\nVariable Explanations\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nNo\nEnd of Season Ranking (1 winning the Finals)\n\n\nTeam\nNBA Team Name\n\n\nG\nGames Played\n\n\nMin\nAverage Minutes Per Game\n\n\nPts\nAverage Points Per Game\n\n\nReb\nAverage Rebounds Per Game\n\n\nAst\nAverage Assists Per Game\n\n\nStl\nAverage Steals Per Game\n\n\nBlk\nAverage Blocks Per Game\n\n\nTo\nAverage Turnovers Per Game\n\n\nPf\nAverage Fouls Per Game\n\n\nDreb\nAverage Defensive Rebounds Per Game\n\n\nOreb\nAverage Offensive Rebounds Per Game\n\n\nFgm-a\nAverage Field Goals Made Per Game - Average Field Goals Attempted Per Game\n\n\n3gm-a\nAverage 3 Pointers Made Per Game - Average 3 Pointers Attempted Per Game\n\n\nFtm-a\nAverage Free Throws Made Per Game - Average Free Throws Attempted Per Game\n\n\nEff\nNBA Efficiency Recap\n\n\nDeff\nEfficiency Recap Difference\n\n\nYear\nYear of the NBA Season\n\n\n\n\n\nData Cleaning\n\nteam_stats_tidy &lt;-\n  team_stats |&gt;\n  mutate(Team = factor(Team))\nteam_stats_tidy &lt;-\n  team_stats_tidy |&gt;\n  mutate(Team = fct_recode(Team,\n                           Bulls = \"Chicago\",\n                           Jazz = \"Utah\",\n                           Suns = \"Phoenix\",\n                           Lakers = \"L.A.Lakers\",\n                           Spurs = \"San Antonio\",\n                           Pacers = \"Indiana\", \n                           Heat = \"Miami\",\n                           Thunder = \"Seattle\", \n                           Hawks = \"Atlanta\", \n                           Knicks = \"New York\",\n                           Cavaliers = \"Cleveland\",\n                           Hornets = \"Charlotte\",\n                           Trailblazers = \"Portland\", \n                           Timberwolves = \"Minnesota\",\n                           Wizards = \"Washington\",\n                           Pistons = \"Detroit\",\n                           Nets = \"New Jersey\",\n                           Bucks = \"Milwaukee\",\n                           Magic = \"Orlando\",\n                           Rockets = \"Houston\",\n                           Celtics = \"Boston\",\n                           `76ers` = \"Philadelphia\",\n                           Kings = \"Sacramento\",\n                           Mavericks = \"Dallas\",\n                           Grizzlies = \"Vancouver\",\n                           Clippers = \"L.A.Clippers\",\n                           Warriors = \"Golden State\",\n                           Raptors = \"Toronto\",\n                           Nuggets = \"Denver\",\n                           Pelicans = \"New Orleans\",\n                           Grizzlies = \"Memphis\",\n                           Nets = \"Brooklyn\",\n                           Thunder = \"Oklahoma City\"))\n\n\nteam_stats_tidy &lt;-\n  team_stats_tidy |&gt;\n  separate(`Fgm-a`,\n           sep = \"-\",\n           into = c(\"FGM\", \"FGA\")) |&gt;\n  separate(`3gm-a`,\n           sep = \"-\",\n           into = c(\"3PM\", \"3PA\")) |&gt;\n  separate(`Ftm-a`,\n           sep = \"-\",\n           into = c(\"FTM\", \"FTA\"))\n\n\nteam_stats_tidy &lt;-\n  team_stats_tidy |&gt;\n  rename(FG_pct = Pct...15,\n         `3P_pct` = Pct...17,\n         FT_pct = Pct...19) |&gt;\n  mutate(FGM = as.numeric(FGM),\n         FGA = as.numeric(FGA),\n         `3PM` = as.numeric(`3PM`),\n         `3PA` = as.numeric(`3PA`),\n         FTM = as.numeric(FTM),\n         FTA = as.numeric(FTA),\n         FG_pct = FG_pct * 100,\n         `3P_pct` = `3P_pct` * 100,\n         FT_pct = FT_pct * 100) |&gt;\n  mutate(Year = str_sub(Year,\n                        6),\n         Year = as.numeric(Year))\n\n\nteam_stats_tidy &lt;-\n  team_stats_tidy |&gt;\n  mutate(Conference = if_else(Team %in% c(\"Celtics\", \"Bucks\", \"Cavaliers\", \"Knicks\", \"76ers\", \"Magic\", \"Heat\", \"Pacers\", \"Bulls\", \"Hawks\", \"Nets\", \"Raptors\", \"Hornets\", \"Wizards\", \"Pistons\"), \"East\", \"West\"),\n         Playoffs = if_else(G &gt; 82, 1, 0)) |&gt;\n  mutate(over100 = factor(if_else(Pts &gt;= 100, \"Yes\", \"No\")))\n\n\n\nCleaned Data Set\nHere is an example of what the data now looks like\n\nkable(head(team_stats_tidy))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo\nTeam\nG\nMin\nPts\nReb\nAst\nStl\nBlk\nTo\nPf\nDreb\nOreb\nFGM\nFGA\nFG_pct\n3PM\n3PA\n3P_pct\nFTM\nFTA\nFT_pct\nEff\nDeff\nYear\nConference\nPlayoffs\nover100\n\n\n\n\n1\nBulls\n103\n48.4\n96.0\n44.1\n23.1\n8.6\n4.3\n13.0\n21.1\n29.2\n14.9\n36.7\n81.7\n44.9\n3.9\n12.0\n32.3\n18.7\n25.2\n74.1\n111.6\n17.5\n1998\nEast\n1\nNo\n\n\n2\nJazz\n102\n48.3\n98.6\n40.8\n24.7\n7.6\n4.8\n14.7\n24.3\n29.5\n11.3\n35.9\n74.3\n48.3\n3.1\n8.4\n36.8\n23.8\n30.9\n76.8\n116.3\n17.5\n1998\nWest\n1\nNo\n\n\n3\nSuns\n86\n48.6\n99.3\n41.9\n25.6\n9.2\n5.3\n14.4\n21.7\n29.8\n12.1\n38.2\n82.0\n46.6\n5.2\n14.7\n35.5\n17.7\n23.6\n74.7\n117.1\n13.6\n1998\nWest\n1\nNo\n\n\n4\nLakers\n95\n48.3\n104.8\n42.9\n24.3\n8.7\n6.8\n14.7\n22.9\n29.7\n13.2\n38.0\n79.1\n48.0\n6.1\n17.3\n35.0\n22.8\n33.7\n67.5\n120.8\n13.2\n1998\nWest\n1\nYes\n\n\n5\nSpurs\n91\n48.4\n92.5\n44.1\n21.9\n6.2\n6.9\n15.3\n21.2\n32.2\n11.9\n35.1\n75.1\n46.8\n3.7\n10.8\n34.4\n18.5\n26.8\n68.8\n108.0\n13.1\n1998\nWest\n1\nNo\n\n\n6\nPacers\n98\n48.4\n95.3\n38.6\n22.4\n7.8\n4.5\n13.6\n23.0\n28.2\n10.4\n35.0\n74.9\n46.8\n5.0\n12.9\n38.7\n20.3\n26.6\n76.3\n109.0\n12.2\n1998\nEast\n1\nNo\n\n\n\n\n\n\n\nBuilding a model\nI decided to use some of the metrics in the data set to predict whether a team would make the playoffs or not. I made a response variables Playoffs by using a condition of if a team played more than 82 games or not in season.\nIn order to make sure that this model isn’t testing using data that knows the answer, we should get rid of both of the No and G variables. We also don’t care about what team it is so we should get rid of the team factor.\n\nteam_stats_tidy &lt;-\n  team_stats_tidy |&gt;\n  select(-(c(\"No\",\"G\", \"Team\")))\n\nAs this is a blog for data visualization, I am only going to choose three predictors that were all found to be relatively significant in a model using all the predictors. One of these is qualitative to demonstrate more visualization. The predictors are Deff, Conference, and FG_pct. Shown below is my model with their coefficients and p values.\n\nplayoffs_mod &lt;- glm(Playoffs ~ Deff + Conference + FG_pct,\n                    data = team_stats_tidy,\n                    family = 'binomial')\nkable(tidy(playoffs_mod))\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.4539997\n3.7403799\n0.1213780\n0.9033917\n\n\nDeff\n0.2665841\n0.0209194\n12.7433723\n0.0000000\n\n\nConferenceWest\n-0.7424358\n0.2228549\n-3.3314764\n0.0008639\n\n\nFG_pct\n-0.0040242\n0.0831379\n-0.0484033\n0.9613948\n\n\n\n\n\n\ngrid &lt;-\n  team_stats_tidy |&gt;\n  data_grid(\n    Deff = seq_range(Deff, n = 60),\n    FG_pct = median(FG_pct),\n    Conference = c(\"West\", \"East\"),\n  )\n\n\naug_stats &lt;-\n  augment(playoffs_mod,\n          newdata = grid,\n          se_fit = TRUE)\naug_stats\n\n# A tibble: 120 × 5\n    Deff FG_pct Conference .fitted .se.fit\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt;\n 1 -26     45.2 East         -6.66   0.548\n 2 -26     45.2 West         -7.40   0.599\n 3 -25.0   45.2 East         -6.40   0.529\n 4 -25.0   45.2 West         -7.14   0.579\n 5 -24.0   45.2 East         -6.13   0.509\n 6 -24.0   45.2 West         -6.88   0.559\n 7 -23.0   45.2 East         -5.87   0.489\n 8 -23.0   45.2 West         -6.61   0.539\n 9 -22.1   45.2 East         -5.61   0.470\n10 -22.1   45.2 West         -6.35   0.520\n# ℹ 110 more rows\n\n\nNext I need to convert the .fitted values to predicted probabilities. Currently they are showing the predicted log odds.\n\naug_stats &lt;-\n  aug_stats |&gt;\n  mutate(.predprob = (exp(.fitted) / (1 + exp(.fitted))))\naug_stats\n\n# A tibble: 120 × 6\n    Deff FG_pct Conference .fitted .se.fit .predprob\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1 -26     45.2 East         -6.66   0.548  0.00128 \n 2 -26     45.2 West         -7.40   0.599  0.000610\n 3 -25.0   45.2 East         -6.40   0.529  0.00166 \n 4 -25.0   45.2 West         -7.14   0.579  0.000793\n 5 -24.0   45.2 East         -6.13   0.509  0.00216 \n 6 -24.0   45.2 West         -6.88   0.559  0.00103 \n 7 -23.0   45.2 East         -5.87   0.489  0.00281 \n 8 -23.0   45.2 West         -6.61   0.539  0.00134 \n 9 -22.1   45.2 East         -5.61   0.470  0.00365 \n10 -22.1   45.2 West         -6.35   0.520  0.00174 \n# ℹ 110 more rows\n\n\nFor my visualization I plan on using the geom_rug feature, so I need to make two separate data frames with teams that made the playoffs and teams that didn’t.\n\nstats_playoffs &lt;-\n  team_stats_tidy |&gt;\n  filter(Playoffs == 1)\n\nstats_nplayoffs &lt;-\n  team_stats_tidy |&gt;\n  filter(Playoffs == 0)\n\n\n\nFigure 1\n\n#|fig-width: 12\naug_stats|&gt;\n  ggplot(aes(x = Deff,\n             y = .predprob)) +\n  geom_line(aes(colour = Conference),\n            linewidth = 1.2,\n            alpha = 0.8) +\n  scale_colour_viridis_d() +\n  geom_rug(data = stats_playoffs,\n           sides = 't',\n           alpha = 0.3,\n           aes(y = Playoffs)) +\n  geom_rug(data = stats_nplayoffs,\n           sides = 'b',\n           alpha = 0.3,\n           aes(y = Playoffs)) +\n  labs(x = \"Efficiency Recap Rating\",\n       y = \"Predicted Probability\",\n       colour = \"Conference\",\n       title = \"Probability of an NBA Team to Make the Playoffs\",\n       caption = \"(based on data from NBA via data world from 1998 - 2022)\") +\n  theme_minimal() +\n    theme(axis.title = element_text(size = 10,),\n        strip.text = element_text(size = 12),\n        plot.title = element_text(size = 16,\n                                  hjust = 0),\n        plot.subtitle = element_text(size = 6),\n        panel.spacing = unit(1, 'lines'),\n        strip.text.x = element_text(color = \"black\", \n                                    face = \"bold\"),\n        strip.background = element_rect(colour = \"black\",\n                                        fill = \"ivory\"),\n        panel.background = element_rect(fill = \"ivory\"),\n        plot.background = element_rect(fill = \"ivory\"),\n        plot.caption = element_text(hjust = 1.5)) +\n  scale_x_continuous(breaks=seq(-30, 40, 10))\n\n\n\n\n\n\nConclusion\nThis plot shows the differences in the probability of a team to make the playoffs dependent on their efficiency recap rating, their median field goal percentage, and their conference. What we can see here is that for a team in the Eastern Conference, they need a lower efficiency recap rating to have an over .5 probability of making the playoffs. This can imply that typically over all the seasons this data has been collected, it is easier to make the playoffs in the Eastern Conference.\n\n\nConnections to Class\nIn this blog, I used many tactics from class. One being the data visualization aspects of using faceting, and grouping. I put the geom_rug on as well and am visualizing my predicted probabilities for each observation. I also used our technique of creating a logistic regression model."
  },
  {
    "objectID": "posts/blog-post-1/blog-post-1.html",
    "href": "posts/blog-post-1/blog-post-1.html",
    "title": "Finding Indicators for Stock Prices",
    "section": "",
    "text": "Introduction:\nAfter finding a data set showing the stock prices of big tech companies, I decided I wanted to investigate to see if there were any correlations to find trends in how their prices perform. This dataset began in 2010 and went to January of 2023 so I decided to cut down the time frame to the most recent five years. I also chose to use the six companies in this dataset with the highest observed closing prices. This allows for less clutter in my visualizations.\n\npricesNew &lt;-\n  prices |&gt;\n  filter(date &gt; '2018-01-01')\n\nprices |&gt;\n  group_by(stock_symbol) |&gt;\n  summarise(max = max(close)) |&gt;\n  arrange(desc(max)) |&gt;\n  head(n = 6)\n\n# A tibble: 6 × 2\n  stock_symbol   max\n  &lt;chr&gt;        &lt;dbl&gt;\n1 NFLX          692.\n2 ADBE          688.\n3 TSLA          410.\n4 META          382.\n5 MSFT          343.\n6 NVDA          334.\n\npricesNew &lt;- \n  pricesNew|&gt;\n  group_by(stock_symbol) |&gt;\n  filter(stock_symbol == c(\"NFLX\", \"ADBE\", \"TSLA\", \"MSFT\", \"NVDA\", \"META\"))\n\n\nPlot 1\n\npricesNew |&gt;\n  ggplot(aes(x = date,\n             y = close)) +\n  geom_line(aes(color = stock_symbol),\n            linewidth = .9) +\n  scale_color_brewer(palette = \"Paired\") + \n  labs(x = \"Date\",\n       y = \"Closing Price ($)\",\n       title = \"Closing Stock Prices from 6 Big Tech Companies, 2018-2023\",\n       color = \"Tickers\",\n       caption = \"(based on data from Yahoo Finance via Kaggle)\") +\n  scale_x_date(date_minor_breaks = \"6 months\",\n               date_breaks = \"1 year\",\n               date_labels = \"%b %y\") +\n  theme(axis.text.x = element_text(angle = 45,\n                                   hjust = 1,\n                                   vjust = 1),\n        plot.title = element_text(size = 16,\n                                  hjust = 0),\n        axis.title.x = element_blank(),\n        axis.title.y = element_text(size = 14))\n\n\n\n\nMy first plot shows the change in price for these six companies and it is easy to see that around January 2022, they all began to take a hit in their price. This begged the question, were there any indicators that this was going to happen? In order to attack this, I decided to investigate the short- and long-term moving averages for each stock, specifically the exponential moving average. An exponential moving average calculates the average of the prices using a weighting multiplier that assigns more weight to later data. In general, when a short-term EMA crosses a long-term EMA, this signifies either a price increase or decrease.\nTo calculate this, I had to make a new data set with the dates extending further back in order to get more observations for the moving averages. I also calculated the average price so it can be used as the y-variable.\n\npricesMA &lt;-\n  prices |&gt;\n  group_by(stock_symbol) |&gt;\n  filter(stock_symbol == c(\"NFLX\", \"ADBE\", \"TSLA\", \"MSFT\", \"NVDA\", \"META\")) |&gt;\n  pivot_longer(cols = 3:6,\n               values_to = \"price\",\n               names_to = \"market\")\n\npricesMA &lt;-\n  pricesMA |&gt;\n  group_by(stock_symbol, date) |&gt;\n  summarise(avgPrice = mean(price))\n\nprices_select &lt;-\n  prices |&gt;\n  filter(stock_symbol == c(\"NFLX\", \"ADBE\", \"TSLA\", \"MSFT\", \"NVDA\", \"META\"))\n\n\n\nPlot 2\n\npricesMA |&gt;\n  ggplot(aes(x = date,\n             y = avgPrice)) +\n  geom_ma(ma_fun = EMA,\n          n = 50,\n          aes(color = \"50 EMA\"))+\n  geom_ma(ma_fun = EMA,\n          n = 200,\n          aes(color = \"200 EMA\")) +\n  facet_wrap(vars(stock_symbol),\n             scales = \"free_y\") +\n    scale_x_date(date_breaks = \"4 year\",\n                 date_labels = \"%b %y\") +\n  labs(y = \"Dollars ($)\",\n       colour = \"Moving Averages\",\n       title = \"Short- and Long-Term EMA Compared to Closing Prices, 2010-2023\",\n       caption = \"(based on data from Yahoo Finance via Kaggle)/nEMAs are calculated from library(tidyquant)\") +\n  theme(axis.text.x = element_text(angle = 45,\n                                   hjust = 1,\n                                   vjust = 1),\n        plot.title = element_text(size = 16,\n                                  hjust = 0),\n        axis.title.x = element_blank(),\n        axis.title.y = element_text(size = 14),\n        panel.spacing = unit(1, \"lines\"),\n        strip.text.x = element_text(color = \"black\", \n                                    face = \"bold\"),\n        strip.background = element_rect(colour = \"black\",\n                                        fill = \"white\"),\n        legend.title = element_blank()) +\n  geom_line(data = prices_select,\n            aes(x = date,\n                y = close),\n            linetype = 1,\n            color = \"black\",\n            alpha = .2)\n\n\n\n\nConclusion:\nIn this plot, I plotted the moving averages against the closing prices from Plot 1 except this time I faceted by each company in order to compare the companies individually. This leads us to our answer to the original question of if there is a trend or indicator of the price decrease. Typically, when a short-term EMA crosses above the long-term EMA, this is a signal that price might continue to move up. On the opposite hand, this means that if short-term EMA crosses below long-term EMA, price might continue to move down. In every company shown, up until the closing price drop, the short-term was above the long-term. Once the gap between them closed, and short-term came closer to long-term, the closing prices began to decrease. In some of the companies, the short-term even dipped below the long term. In conclusion, using moving averages can be a good indicator for these companies prices going down, but likely isn’t the root of the scenario. That can be left to many other more complex factors.\nConnections to Class: I used many techniques we used in class. The three main ones were changing theme elements, faceting, and using a color scale."
  },
  {
    "objectID": "posts/blog-post-2/blog-post-2.html",
    "href": "posts/blog-post-2/blog-post-2.html",
    "title": "Is There Bias Towards the Big 6?",
    "section": "",
    "text": "Introduction\nWhile scrolling through the Tidy Tuesday data, I found a data set of the Premier League Matches from 2021-2022. In this data, they included team goals and results for both the first half and the second half. They also included the referee, team shots, team shots on target, team fouls, team yellow cards, and team red cards. When thinking about the variables that this data had, I thought of the question is there bias towards the “big 6”. For those of you who don’t follow the Premier League, the big 6 consists of Arsenal, Liverpool, Manchester United, Manchester City, Chelsea, and Tottenham. My goal was to see if there was a notable difference in the number of fouls produced for the big six vs their opponents.\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(knitr)\ntheme_set(theme_minimal())\nsoccer &lt;- read_csv(here(\"data/soccer.csv\"))\n\n\n\nData Cleaning\nMy first step was to get rid of the halftime data, the shots data, the goals data, the winners data, and the corners data because I wouldn’t be needing it for my analysis. I chose to keep the referee variable and cards variables just in case I want to use it later. I also wanted to make the variable names easier to interpret.\n\nsoccerTidy &lt;-\n  soccer |&gt;\n  select(-starts_with(\"HT\")) |&gt;\n  select(-(4:6)) |&gt;\n  select(-(5:8)) |&gt;\n  select(-(7:8)) |&gt;\n  rename('HomeFouls' = HF) |&gt;\n  rename('AwayFouls' = AF) |&gt;\n  rename('HomeYellow' = HY) |&gt;\n  rename('AwayYellow' = AY) |&gt;\n  rename('HomeRed' = HR) |&gt;\n  rename('AwayRed' = AR)\n\nBelow is a sample of what the data now looks like.\n\nkable(head(soccerTidy))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nHomeTeam\nAwayTeam\nReferee\nHomeFouls\nAwayFouls\nHomeYellow\nAwayYellow\nHomeRed\nAwayRed\n\n\n\n\n13/08/2021\nBrentford\nArsenal\nM Oliver\n12\n8\n0\n0\n0\n0\n\n\n14/08/2021\nMan United\nLeeds\nP Tierney\n11\n9\n1\n2\n0\n0\n\n\n14/08/2021\nBurnley\nBrighton\nD Coote\n10\n7\n2\n1\n0\n0\n\n\n14/08/2021\nChelsea\nCrystal Palace\nJ Moss\n15\n11\n0\n0\n0\n0\n\n\n14/08/2021\nEverton\nSouthampton\nA Madley\n13\n15\n2\n0\n0\n0\n\n\n14/08/2021\nLeicester\nWolves\nC Pawson\n6\n10\n1\n2\n0\n0\n\n\n\n\n\n\n\nPreparing for Visuals\nNext I needed to make multiple data frames, a home and away for the big 6 which could then be joined so I could compare how many fouls they got when versing non big 6 opponents.\nSide Note: All of this was a huge pain, and there is likely a much easier way to do this. Below is all the code of making data frames and joining them together until I have something that works for showing the distribution.\n\nbig6_Home &lt;-\n  soccerTidy |&gt;\n  filter(HomeTeam %in% c('Arsenal', 'Liverpool', 'Man United', 'Man City', 'Chelsea', 'Tottenham')) |&gt;\n    filter(!(AwayTeam %in% c('Arsenal', 'Liverpool', 'Man United', 'Man City', 'Chelsea', 'Tottenham')))\n\nbig6_Away &lt;-\n  soccerTidy |&gt;\n  filter(AwayTeam %in% c('Arsenal', 'Liverpool', 'Man United', 'Man City', 'Chelsea', 'Tottenham')) |&gt;\n  filter(!(HomeTeam %in% c('Arsenal', 'Liverpool', 'Man United', 'Man City', 'Chelsea', 'Tottenham')))\n\n\nbig6_fouls &lt;-\n  big6_Home |&gt;\n    group_by(HomeTeam) |&gt;\n  mutate('Big_6' = 'Yes') |&gt;\n  select(HomeTeam, HomeFouls, Big_6)\n\nopp_fouls &lt;-\n  big6_Home |&gt;\n    group_by(HomeTeam) |&gt;\n  mutate('Big_6' = 'No') |&gt;\n  select(HomeTeam, AwayFouls, Big_6)\n\nhome_fouls &lt;-\n  bind_rows(big6_fouls, opp_fouls) |&gt;\n  pivot_longer(cols = c('HomeFouls', 'AwayFouls'),\n               names_to = 'Location',\n               values_to = 'Fouls') |&gt;\n  select(-(Location)) |&gt;\n  na.omit() |&gt;\n  mutate(Location = 'Home')\n\n\nAbig6_fouls &lt;-\n  big6_Away |&gt;\n    group_by(AwayTeam) |&gt;\n  mutate('Big_6' = 'Yes') |&gt;\n  select(AwayTeam, AwayFouls, Big_6)\n\nAopp_fouls &lt;-\n  big6_Away |&gt;\n    group_by(AwayTeam) |&gt;\n  mutate('Big_6' = 'No') |&gt;\n  select(AwayTeam, HomeFouls, Big_6)\n\naway_fouls &lt;-\n  bind_rows(Abig6_fouls, Aopp_fouls) |&gt;\n  pivot_longer(cols = c('HomeFouls', 'AwayFouls'),\n               names_to = 'Location',\n               values_to = 'Fouls') |&gt;\n  select(-(Location)) |&gt;\n  na.omit() |&gt;\n  mutate(Location = 'Away') |&gt;\n  rename('HomeTeam' = AwayTeam)\n\n\nfouls_df &lt;-\n  bind_rows(home_fouls, away_fouls) |&gt;\n  mutate(Big_6 = as_factor(Big_6))\n\n\n\nPlot 1\n\nggplot(data = fouls_df,\n       aes(x = HomeTeam,\n           y = Fouls,\n           fill = Big_6)) +\n  geom_boxplot(position = position_dodge(width = 1)) +\n  facet_wrap(vars(Location)) +\n  theme(axis.title = element_text(size = 14,),\n        strip.text = element_text(size = 12),\n        axis.title.y = element_blank(),\n        plot.title = element_text(size = 16,\n                                  hjust = 0),\n        panel.spacing = unit(2, 'lines')) +\n  labs(title = 'Distribution of Fouls in Big 6 Games Both Home and Away',\n       caption = '(based on data from Evan Gower via Kaggle)',\n       y = 'Number of Fouls',\n       fill = 'In Big 6?') +\n  coord_flip() +\n  scale_fill_brewer(palette = 'Pastel2')\n\n\n\n\nThis plot shows the distribution of fouls for all the Big 6 teams both home and away as well as their opponents. When I had this idea, I was expecting to see a somewhat lower distribution for all of the Big 6 teams relative to their opponent, but I was surprised. For the most part, the fouls seam relatively even with the biggest advantage appearing to be Arsenal in home games. Below I created a table to show the average number of fouls per game for each Big 6 team.\n\n\nTable 1\n\ntb6 &lt;-\nfouls_df |&gt;\n  filter(Big_6 == 'Yes') |&gt;\n  group_by(HomeTeam) |&gt;\n  summarise('Fouls per Game for Big 6 Teams' = mean(Fouls))\n\ntb &lt;-\n  fouls_df |&gt;\n  filter(Big_6 == 'No') |&gt;\n  group_by(HomeTeam) |&gt;\n  summarise('Fouls per Game for Opponents' = mean(Fouls)) |&gt;\n  select(-HomeTeam)\n\ntb &lt;-\n  bind_cols(tb6, tb)\n\nkable(tb)\n\n\n\n\n\n\n\n\n\nHomeTeam\nFouls per Game for Big 6 Teams\nFouls per Game for Opponents\n\n\n\n\nArsenal\n9.285714\n10.321429\n\n\nChelsea\n11.214286\n10.750000\n\n\nLiverpool\n9.214286\n8.000000\n\n\nMan City\n8.321429\n8.607143\n\n\nMan United\n10.357143\n8.464286\n\n\nTottenham\n10.357143\n10.535714\n\n\n\n\n\nThis table shows that only a few Big 6 teams average less than their opponents, those being Arsenal, Man City, and Tottenham. I would say that for the most part, this shows that there isn’t bias towards the Big 6.\n\n\nConclusion\nUltimately, it seems as if there isn’t bias towards the Big 6 in terms of the number of fouls for them and for their opponents. As well, I was hoping to create some form of model to predict this, but it wasn’t a good possibility with this data set. Looking to the future, I likely would have added in another question and made a new visual as this one is super busy and not the best to interpret, but the process to get there took a while so I’ve decided to keep it. I might come back to this data in the future to try and answer a different question.\n\n\nConnections to Class\nI used many techniques we’ve learned in class in this post. Some being the dplyr tools such as filtering and selecting, binding and merging tables. I also used most of the ggplot techniques from faceting, flipping the coordinates, and adjusting the theme elements to make it look nicer."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DataVizBlog",
    "section": "",
    "text": "Predicting if a NBA Team Will Make the Playoffs\n\n\n\n\n\n\n\nsports\n\n\ncode\n\n\nanalysis\n\n\ndata wrangling\n\n\n\n\n\n\n\n\n\n\n\nMar 6, 2024\n\n\nEric Seltzer\n\n\n\n\n\n\n  \n\n\n\n\nIs There Bias Towards the Big 6?\n\n\n\n\n\n\n\nsports\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nFeb 21, 2024\n\n\nEric Seltzer\n\n\n\n\n\n\n  \n\n\n\n\nFinding Indicators for Stock Prices\n\n\n\n\n\n\n\nfinance\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nFeb 9, 2024\n\n\nEric Seltzer\n\n\n\n\n\n\nNo matching items"
  }
]