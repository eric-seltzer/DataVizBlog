[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/tech-stock/tech-stock.html",
    "href": "posts/tech-stock/tech-stock.html",
    "title": "Finding Indicators for Stock Prices",
    "section": "",
    "text": "Introduction:\nAfter finding a data set showing the stock prices of big tech companies, I decided I wanted to investigate to see if there were any correlations to find trends in how their prices perform. This dataset began in 2010 and went to January of 2023 so I decided to cut down the time frame to the most recent five years. I also chose to use the six companies in this dataset with the highest observed closing prices. This allows for less clutter in my visualizations.\n\npricesNew &lt;-\n  prices |&gt;\n  filter(date &gt; '2018-01-01')\n\nprices |&gt;\n  group_by(stock_symbol) |&gt;\n  summarise(max = max(close)) |&gt;\n  arrange(desc(max)) |&gt;\n  head(n = 6)\n\n# A tibble: 6 × 2\n  stock_symbol   max\n  &lt;chr&gt;        &lt;dbl&gt;\n1 NFLX          692.\n2 ADBE          688.\n3 TSLA          410.\n4 META          382.\n5 MSFT          343.\n6 NVDA          334.\n\npricesNew &lt;- \n  pricesNew|&gt;\n  group_by(stock_symbol) |&gt;\n  filter(stock_symbol == c(\"NFLX\", \"ADBE\", \"TSLA\", \"MSFT\", \"NVDA\", \"META\"))\n\n\nPlot 1\n\npricesNew |&gt;\n  ggplot(aes(x = date,\n             y = close)) +\n  geom_line(aes(color = stock_symbol),\n            linewidth = .9) +\n  scale_color_brewer(palette = \"Paired\") + \n  labs(x = \"Date\",\n       y = \"Closing Price ($)\",\n       title = \"Closing Stock Prices from 6 Big Tech Companies, 2018-2023\",\n       color = \"Tickers\",\n       caption = \"(based on data from Yahoo Finance via Kaggle)\") +\n  scale_x_date(date_minor_breaks = \"6 months\",\n               date_breaks = \"1 year\",\n               date_labels = \"%b %y\") +\n  theme(axis.text.x = element_text(angle = 45,\n                                   hjust = 1,\n                                   vjust = 1),\n        plot.title = element_text(size = 16,\n                                  hjust = 0),\n        axis.title.x = element_blank(),\n        axis.title.y = element_text(size = 14))\n\n\n\n\nMy first plot shows the change in price for these six companies and it is easy to see that around January 2022, they all began to take a hit in their price. This begged the question, were there any indicators that this was going to happen? In order to attack this, I decided to investigate the short- and long-term moving averages for each stock, specifically the exponential moving average. An exponential moving average calculates the average of the prices using a weighting multiplier that assigns more weight to later data. In general, when a short-term EMA crosses a long-term EMA, this signifies either a price increase or decrease.\nTo calculate this, I had to make a new data set with the dates extending further back in order to get more observations for the moving averages. I also calculated the average price so it can be used as the y-variable.\n\npricesMA &lt;-\n  prices |&gt;\n  group_by(stock_symbol) |&gt;\n  filter(stock_symbol == c(\"NFLX\", \"ADBE\", \"TSLA\", \"MSFT\", \"NVDA\", \"META\")) |&gt;\n  pivot_longer(cols = 3:6,\n               values_to = \"price\",\n               names_to = \"market\")\n\npricesMA &lt;-\n  pricesMA |&gt;\n  group_by(stock_symbol, date) |&gt;\n  summarise(avgPrice = mean(price))\n\nprices_select &lt;-\n  prices |&gt;\n  filter(stock_symbol == c(\"NFLX\", \"ADBE\", \"TSLA\", \"MSFT\", \"NVDA\", \"META\"))\n\n\n\nPlot 2\n\npricesMA |&gt;\n  ggplot(aes(x = date,\n             y = avgPrice)) +\n  geom_ma(ma_fun = EMA,\n          n = 50,\n          aes(color = \"50 EMA\"))+\n  geom_ma(ma_fun = EMA,\n          n = 200,\n          aes(color = \"200 EMA\")) +\n  facet_wrap(vars(stock_symbol),\n             scales = \"free_y\") +\n    scale_x_date(date_breaks = \"4 year\",\n                 date_labels = \"%b %y\") +\n  labs(y = \"Dollars ($)\",\n       colour = \"Moving Averages\",\n       title = \"Short- and Long-Term EMA Compared to Closing Prices, 2010-2023\",\n       caption = \"(based on data from Yahoo Finance via Kaggle)/nEMAs are calculated from library(tidyquant)\") +\n  theme(axis.text.x = element_text(angle = 45,\n                                   hjust = 1,\n                                   vjust = 1),\n        plot.title = element_text(size = 16,\n                                  hjust = 0),\n        axis.title.x = element_blank(),\n        axis.title.y = element_text(size = 14),\n        panel.spacing = unit(1, \"lines\"),\n        strip.text.x = element_text(color = \"black\", \n                                    face = \"bold\"),\n        strip.background = element_rect(colour = \"black\",\n                                        fill = \"white\"),\n        legend.title = element_blank()) +\n  geom_line(data = prices_select,\n            aes(x = date,\n                y = close),\n            linetype = 1,\n            color = \"black\",\n            alpha = .2)\n\n\n\n\nConclusion:\nIn this plot, I plotted the moving averages against the closing prices from Plot 1 except this time I faceted by each company in order to compare the companies individually. This leads us to our answer to the original question of if there is a trend or indicator of the price decrease. Typically, when a short-term EMA crosses above the long-term EMA, this is a signal that price might continue to move up. On the opposite hand, this means that if short-term EMA crosses below long-term EMA, price might continue to move down. In every company shown, up until the closing price drop, the short-term was above the long-term. Once the gap between them closed, and short-term came closer to long-term, the closing prices began to decrease. In some of the companies, the short-term even dipped below the long term. In conclusion, using moving averages can be a good indicator for these companies prices going down, but likely isn’t the root of the scenario. That can be left to many other more complex factors.\nConnections to Class: I used many techniques we used in class. The three main ones were changing theme elements, faceting, and using a color scale."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DataVizBlog",
    "section": "",
    "text": "Is There Bias Towards the Big 6?\n\n\n\n\n\n\n\nsports\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nFeb 21, 2024\n\n\nEric Seltzer\n\n\n\n\n\n\n  \n\n\n\n\nFinding Indicators for Stock Prices\n\n\n\n\n\n\n\nfinance\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nFeb 9, 2024\n\n\nEric Seltzer\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog-post-2.html",
    "href": "blog-post-2.html",
    "title": "Is there bias towards the Big 6?",
    "section": "",
    "text": "Introduction\nWhile scrolling through the Tidy Tuesday data, I found a data set of the Premier League Matches from 2021-2022. In this data, they included team goals and results for both the first half and the second half. They also included the referee, team shots, team shots on target, team fouls, team yellow cards, and team red cards. When thinking about the variables that this data had, I thought of the question is there bias towards the “big 6”. For those of you who don’t follow the Premier League, the big 6 consists of Arsenal, Liverpool, Manchester United, Manchester City, Chelsea, and Tottenham. My goal was to see if there was a notable difference in the number of fouls and cards produced for the big six vs the other 14 teams.\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(knitr)\ntheme_set(theme_minimal())\nsoccer &lt;- read_csv(here(\"data/soccer.csv\"))\n\n\n\nData Cleaning\nMy first step was to get rid of the halftime data, the shots data, the goals data, the winners data, and the corners data because I wouldn’t be needing it for my analysis. I chose to keep the referee variable and cards variables just in case I want to use it later.I also wanted to make the variable names easier to interpret.\n\nsoccerTidy &lt;-\n  soccer |&gt;\n  select(-starts_with(\"HT\")) |&gt;\n  select(-(4:6)) |&gt;\n  select(-(5:8)) |&gt;\n  select(-(7:8)) |&gt;\n  rename('HomeFouls' = HF) |&gt;\n  rename('AwayFouls' = AF) |&gt;\n  rename('HomeYellow' = HY) |&gt;\n  rename('AwayYellow' = AY) |&gt;\n  rename('HomeRed' = HR) |&gt;\n  rename('AwayRed' = AR)\n\nBelow is a sample of what the data now looks like.\n\nkable(head(soccerTidy))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nHomeTeam\nAwayTeam\nReferee\nHomeFouls\nAwayFouls\nHomeYellow\nAwayYellow\nHomeRed\nAwayRed\n\n\n\n\n13/08/2021\nBrentford\nArsenal\nM Oliver\n12\n8\n0\n0\n0\n0\n\n\n14/08/2021\nMan United\nLeeds\nP Tierney\n11\n9\n1\n2\n0\n0\n\n\n14/08/2021\nBurnley\nBrighton\nD Coote\n10\n7\n2\n1\n0\n0\n\n\n14/08/2021\nChelsea\nCrystal Palace\nJ Moss\n15\n11\n0\n0\n0\n0\n\n\n14/08/2021\nEverton\nSouthampton\nA Madley\n13\n15\n2\n0\n0\n0\n\n\n14/08/2021\nLeicester\nWolves\nC Pawson\n6\n10\n1\n2\n0\n0\n\n\n\n\n\n\n\nPreparing for Visuals\nNext I needed to make two different data frames, a home and away for the big 6 which could then be joined so I could compare how many fouls they got when versing non big 6 opponents.\nSide Note: All of this was a huge pain, and there is likely a much easier way to do this. Below is all the code of making data frames and joining them together until I have something that works for showing the distribution.\n\nbig6_Home &lt;-\n  soccerTidy |&gt;\n  filter(HomeTeam %in% c('Arsenal', 'Liverpool', 'Man United', 'Man City', 'Chelsea', 'Tottenham')) |&gt;\n    filter(!(AwayTeam %in% c('Arsenal', 'Liverpool', 'Man United', 'Man City', 'Chelsea', 'Tottenham')))\n\nbig6_Away &lt;-\n  soccerTidy |&gt;\n  filter(AwayTeam %in% c('Arsenal', 'Liverpool', 'Man United', 'Man City', 'Chelsea', 'Tottenham')) |&gt;\n  filter(!(HomeTeam %in% c('Arsenal', 'Liverpool', 'Man United', 'Man City', 'Chelsea', 'Tottenham')))\n\n\nbig6_fouls &lt;-\n  big6_Home |&gt;\n    group_by(HomeTeam) |&gt;\n  mutate('Big_6' = 'Yes') |&gt;\n  select(HomeTeam, HomeFouls, Big_6)\n\nopp_fouls &lt;-\n  big6_Home |&gt;\n    group_by(HomeTeam) |&gt;\n  mutate('Big_6' = 'No') |&gt;\n  select(HomeTeam, AwayFouls, Big_6)\n\nhome_fouls &lt;-\n  bind_rows(big6_fouls, opp_fouls) |&gt;\n  pivot_longer(cols = c('HomeFouls', 'AwayFouls'),\n               names_to = 'Location',\n               values_to = 'Fouls') |&gt;\n  select(-(Location)) |&gt;\n  na.omit() |&gt;\n  mutate(Location = 'Home')\n\n\nAbig6_fouls &lt;-\n  big6_Away |&gt;\n    group_by(AwayTeam) |&gt;\n  mutate('Big_6' = 'Yes') |&gt;\n  select(AwayTeam, AwayFouls, Big_6)\n\nAopp_fouls &lt;-\n  big6_Away |&gt;\n    group_by(AwayTeam) |&gt;\n  mutate('Big_6' = 'No') |&gt;\n  select(AwayTeam, HomeFouls, Big_6)\n\naway_fouls &lt;-\n  bind_rows(Abig6_fouls, Aopp_fouls) |&gt;\n  pivot_longer(cols = c('HomeFouls', 'AwayFouls'),\n               names_to = 'Location',\n               values_to = 'Fouls') |&gt;\n  select(-(Location)) |&gt;\n  na.omit() |&gt;\n  mutate(Location = 'Away') |&gt;\n  rename('HomeTeam' = AwayTeam)\n\n\nfouls_df &lt;-\n  bind_rows(home_fouls, away_fouls) |&gt;\n  mutate(Big_6 = as_factor(Big_6))\n\n\n\nPlot 1\n\nggplot(data = fouls_df,\n       aes(x = HomeTeam,\n           y = Fouls,\n           fill = Big_6)) +\n  geom_boxplot(position = position_dodge(width = 1)) +\n  facet_wrap(vars(Location)) +\n  theme(axis.title = element_text(size = 14,),\n        strip.text = element_text(size = 12),\n        axis.title.y = element_blank(),\n        plot.title = element_text(size = 16,\n                                  hjust = 0),\n        panel.spacing = unit(2, 'lines')) +\n  labs(title = 'Distribution of Fouls in Big 6 Games Both Home and Away',\n       caption = '(based on data from Evan Gower via Kaggle)',\n       y = 'Number of Fouls',\n       fill = 'In Big 6?') +\n  coord_flip() +\n  scale_fill_brewer(palette = 'Pastel2')\n\n\n\n\nThis plot shows the distribution of fouls for all the Big 6 teams both home and away as well as their opponents. When I had this idea, I was expecting to see a somewhat lower distribution for all of the Big 6 teams relative to their opponent, but I was surprised. For the most part, the fouls seam relatively even with the biggest advantage appearing to be Arsenal in home games. Below I created a table to show the average number of fouls per game for each Big 6 team. # Table 1\n\ntb6 &lt;-\nfouls_df |&gt;\n  filter(Big_6 == 'Yes') |&gt;\n  group_by(HomeTeam) |&gt;\n  summarise('Fouls per Game for Big 6 Teams' = mean(Fouls))\n\ntb &lt;-\n  fouls_df |&gt;\n  filter(Big_6 == 'No') |&gt;\n  group_by(HomeTeam) |&gt;\n  summarise('Fouls per Game for Opponents' = mean(Fouls)) |&gt;\n  select(-HomeTeam)\n\ntb &lt;-\n  bind_cols(tb6, tb)\n\nkable(tb)\n\n\n\n\n\n\n\n\n\nHomeTeam\nFouls per Game for Big 6 Teams\nFouls per Game for Opponents\n\n\n\n\nArsenal\n9.285714\n10.321429\n\n\nChelsea\n11.214286\n10.750000\n\n\nLiverpool\n9.214286\n8.000000\n\n\nMan City\n8.321429\n8.607143\n\n\nMan United\n10.357143\n8.464286\n\n\nTottenham\n10.357143\n10.535714\n\n\n\n\n\nThis table shows that only a few Big 6 teams average less than their opponents, those being Arsenal, Man City, and Tottenham. I would say that for the most part, this shows that there isn’t bias towards the Big 6."
  },
  {
    "objectID": "posts/blog-post-1/blog-post-1.html",
    "href": "posts/blog-post-1/blog-post-1.html",
    "title": "Finding Indicators for Stock Prices",
    "section": "",
    "text": "Introduction:\nAfter finding a data set showing the stock prices of big tech companies, I decided I wanted to investigate to see if there were any correlations to find trends in how their prices perform. This dataset began in 2010 and went to January of 2023 so I decided to cut down the time frame to the most recent five years. I also chose to use the six companies in this dataset with the highest observed closing prices. This allows for less clutter in my visualizations.\n\npricesNew &lt;-\n  prices |&gt;\n  filter(date &gt; '2018-01-01')\n\nprices |&gt;\n  group_by(stock_symbol) |&gt;\n  summarise(max = max(close)) |&gt;\n  arrange(desc(max)) |&gt;\n  head(n = 6)\n\n# A tibble: 6 × 2\n  stock_symbol   max\n  &lt;chr&gt;        &lt;dbl&gt;\n1 NFLX          692.\n2 ADBE          688.\n3 TSLA          410.\n4 META          382.\n5 MSFT          343.\n6 NVDA          334.\n\npricesNew &lt;- \n  pricesNew|&gt;\n  group_by(stock_symbol) |&gt;\n  filter(stock_symbol == c(\"NFLX\", \"ADBE\", \"TSLA\", \"MSFT\", \"NVDA\", \"META\"))\n\n\nPlot 1\n\npricesNew |&gt;\n  ggplot(aes(x = date,\n             y = close)) +\n  geom_line(aes(color = stock_symbol),\n            linewidth = .9) +\n  scale_color_brewer(palette = \"Paired\") + \n  labs(x = \"Date\",\n       y = \"Closing Price ($)\",\n       title = \"Closing Stock Prices from 6 Big Tech Companies, 2018-2023\",\n       color = \"Tickers\",\n       caption = \"(based on data from Yahoo Finance via Kaggle)\") +\n  scale_x_date(date_minor_breaks = \"6 months\",\n               date_breaks = \"1 year\",\n               date_labels = \"%b %y\") +\n  theme(axis.text.x = element_text(angle = 45,\n                                   hjust = 1,\n                                   vjust = 1),\n        plot.title = element_text(size = 16,\n                                  hjust = 0),\n        axis.title.x = element_blank(),\n        axis.title.y = element_text(size = 14))\n\n\n\n\nMy first plot shows the change in price for these six companies and it is easy to see that around January 2022, they all began to take a hit in their price. This begged the question, were there any indicators that this was going to happen? In order to attack this, I decided to investigate the short- and long-term moving averages for each stock, specifically the exponential moving average. An exponential moving average calculates the average of the prices using a weighting multiplier that assigns more weight to later data. In general, when a short-term EMA crosses a long-term EMA, this signifies either a price increase or decrease.\nTo calculate this, I had to make a new data set with the dates extending further back in order to get more observations for the moving averages. I also calculated the average price so it can be used as the y-variable.\n\npricesMA &lt;-\n  prices |&gt;\n  group_by(stock_symbol) |&gt;\n  filter(stock_symbol == c(\"NFLX\", \"ADBE\", \"TSLA\", \"MSFT\", \"NVDA\", \"META\")) |&gt;\n  pivot_longer(cols = 3:6,\n               values_to = \"price\",\n               names_to = \"market\")\n\npricesMA &lt;-\n  pricesMA |&gt;\n  group_by(stock_symbol, date) |&gt;\n  summarise(avgPrice = mean(price))\n\nprices_select &lt;-\n  prices |&gt;\n  filter(stock_symbol == c(\"NFLX\", \"ADBE\", \"TSLA\", \"MSFT\", \"NVDA\", \"META\"))\n\n\n\nPlot 2\n\npricesMA |&gt;\n  ggplot(aes(x = date,\n             y = avgPrice)) +\n  geom_ma(ma_fun = EMA,\n          n = 50,\n          aes(color = \"50 EMA\"))+\n  geom_ma(ma_fun = EMA,\n          n = 200,\n          aes(color = \"200 EMA\")) +\n  facet_wrap(vars(stock_symbol),\n             scales = \"free_y\") +\n    scale_x_date(date_breaks = \"4 year\",\n                 date_labels = \"%b %y\") +\n  labs(y = \"Dollars ($)\",\n       colour = \"Moving Averages\",\n       title = \"Short- and Long-Term EMA Compared to Closing Prices, 2010-2023\",\n       caption = \"(based on data from Yahoo Finance via Kaggle)/nEMAs are calculated from library(tidyquant)\") +\n  theme(axis.text.x = element_text(angle = 45,\n                                   hjust = 1,\n                                   vjust = 1),\n        plot.title = element_text(size = 16,\n                                  hjust = 0),\n        axis.title.x = element_blank(),\n        axis.title.y = element_text(size = 14),\n        panel.spacing = unit(1, \"lines\"),\n        strip.text.x = element_text(color = \"black\", \n                                    face = \"bold\"),\n        strip.background = element_rect(colour = \"black\",\n                                        fill = \"white\"),\n        legend.title = element_blank()) +\n  geom_line(data = prices_select,\n            aes(x = date,\n                y = close),\n            linetype = 1,\n            color = \"black\",\n            alpha = .2)\n\n\n\n\nConclusion:\nIn this plot, I plotted the moving averages against the closing prices from Plot 1 except this time I faceted by each company in order to compare the companies individually. This leads us to our answer to the original question of if there is a trend or indicator of the price decrease. Typically, when a short-term EMA crosses above the long-term EMA, this is a signal that price might continue to move up. On the opposite hand, this means that if short-term EMA crosses below long-term EMA, price might continue to move down. In every company shown, up until the closing price drop, the short-term was above the long-term. Once the gap between them closed, and short-term came closer to long-term, the closing prices began to decrease. In some of the companies, the short-term even dipped below the long term. In conclusion, using moving averages can be a good indicator for these companies prices going down, but likely isn’t the root of the scenario. That can be left to many other more complex factors.\nConnections to Class: I used many techniques we used in class. The three main ones were changing theme elements, faceting, and using a color scale."
  },
  {
    "objectID": "posts/blog-post-2/blog-post-2.html",
    "href": "posts/blog-post-2/blog-post-2.html",
    "title": "Is There Bias Towards the Big 6?",
    "section": "",
    "text": "Introduction\nWhile scrolling through the Tidy Tuesday data, I found a data set of the Premier League Matches from 2021-2022. In this data, they included team goals and results for both the first half and the second half. They also included the referee, team shots, team shots on target, team fouls, team yellow cards, and team red cards. When thinking about the variables that this data had, I thought of the question is there bias towards the “big 6”. For those of you who don’t follow the Premier League, the big 6 consists of Arsenal, Liverpool, Manchester United, Manchester City, Chelsea, and Tottenham. My goal was to see if there was a notable difference in the number of fouls produced for the big six vs their opponents.\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(knitr)\ntheme_set(theme_minimal())\nsoccer &lt;- read_csv(here(\"data/soccer.csv\"))\n\n\n\nData Cleaning\nMy first step was to get rid of the halftime data, the shots data, the goals data, the winners data, and the corners data because I wouldn’t be needing it for my analysis. I chose to keep the referee variable and cards variables just in case I want to use it later. I also wanted to make the variable names easier to interpret.\n\nsoccerTidy &lt;-\n  soccer |&gt;\n  select(-starts_with(\"HT\")) |&gt;\n  select(-(4:6)) |&gt;\n  select(-(5:8)) |&gt;\n  select(-(7:8)) |&gt;\n  rename('HomeFouls' = HF) |&gt;\n  rename('AwayFouls' = AF) |&gt;\n  rename('HomeYellow' = HY) |&gt;\n  rename('AwayYellow' = AY) |&gt;\n  rename('HomeRed' = HR) |&gt;\n  rename('AwayRed' = AR)\n\nBelow is a sample of what the data now looks like.\n\nkable(head(soccerTidy))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nHomeTeam\nAwayTeam\nReferee\nHomeFouls\nAwayFouls\nHomeYellow\nAwayYellow\nHomeRed\nAwayRed\n\n\n\n\n13/08/2021\nBrentford\nArsenal\nM Oliver\n12\n8\n0\n0\n0\n0\n\n\n14/08/2021\nMan United\nLeeds\nP Tierney\n11\n9\n1\n2\n0\n0\n\n\n14/08/2021\nBurnley\nBrighton\nD Coote\n10\n7\n2\n1\n0\n0\n\n\n14/08/2021\nChelsea\nCrystal Palace\nJ Moss\n15\n11\n0\n0\n0\n0\n\n\n14/08/2021\nEverton\nSouthampton\nA Madley\n13\n15\n2\n0\n0\n0\n\n\n14/08/2021\nLeicester\nWolves\nC Pawson\n6\n10\n1\n2\n0\n0\n\n\n\n\n\n\n\nPreparing for Visuals\nNext I needed to make multiple data frames, a home and away for the big 6 which could then be joined so I could compare how many fouls they got when versing non big 6 opponents.\nSide Note: All of this was a huge pain, and there is likely a much easier way to do this. Below is all the code of making data frames and joining them together until I have something that works for showing the distribution.\n\nbig6_Home &lt;-\n  soccerTidy |&gt;\n  filter(HomeTeam %in% c('Arsenal', 'Liverpool', 'Man United', 'Man City', 'Chelsea', 'Tottenham')) |&gt;\n    filter(!(AwayTeam %in% c('Arsenal', 'Liverpool', 'Man United', 'Man City', 'Chelsea', 'Tottenham')))\n\nbig6_Away &lt;-\n  soccerTidy |&gt;\n  filter(AwayTeam %in% c('Arsenal', 'Liverpool', 'Man United', 'Man City', 'Chelsea', 'Tottenham')) |&gt;\n  filter(!(HomeTeam %in% c('Arsenal', 'Liverpool', 'Man United', 'Man City', 'Chelsea', 'Tottenham')))\n\n\nbig6_fouls &lt;-\n  big6_Home |&gt;\n    group_by(HomeTeam) |&gt;\n  mutate('Big_6' = 'Yes') |&gt;\n  select(HomeTeam, HomeFouls, Big_6)\n\nopp_fouls &lt;-\n  big6_Home |&gt;\n    group_by(HomeTeam) |&gt;\n  mutate('Big_6' = 'No') |&gt;\n  select(HomeTeam, AwayFouls, Big_6)\n\nhome_fouls &lt;-\n  bind_rows(big6_fouls, opp_fouls) |&gt;\n  pivot_longer(cols = c('HomeFouls', 'AwayFouls'),\n               names_to = 'Location',\n               values_to = 'Fouls') |&gt;\n  select(-(Location)) |&gt;\n  na.omit() |&gt;\n  mutate(Location = 'Home')\n\n\nAbig6_fouls &lt;-\n  big6_Away |&gt;\n    group_by(AwayTeam) |&gt;\n  mutate('Big_6' = 'Yes') |&gt;\n  select(AwayTeam, AwayFouls, Big_6)\n\nAopp_fouls &lt;-\n  big6_Away |&gt;\n    group_by(AwayTeam) |&gt;\n  mutate('Big_6' = 'No') |&gt;\n  select(AwayTeam, HomeFouls, Big_6)\n\naway_fouls &lt;-\n  bind_rows(Abig6_fouls, Aopp_fouls) |&gt;\n  pivot_longer(cols = c('HomeFouls', 'AwayFouls'),\n               names_to = 'Location',\n               values_to = 'Fouls') |&gt;\n  select(-(Location)) |&gt;\n  na.omit() |&gt;\n  mutate(Location = 'Away') |&gt;\n  rename('HomeTeam' = AwayTeam)\n\n\nfouls_df &lt;-\n  bind_rows(home_fouls, away_fouls) |&gt;\n  mutate(Big_6 = as_factor(Big_6))\n\n\n\nPlot 1\n\nggplot(data = fouls_df,\n       aes(x = HomeTeam,\n           y = Fouls,\n           fill = Big_6)) +\n  geom_boxplot(position = position_dodge(width = 1)) +\n  facet_wrap(vars(Location)) +\n  theme(axis.title = element_text(size = 14,),\n        strip.text = element_text(size = 12),\n        axis.title.y = element_blank(),\n        plot.title = element_text(size = 16,\n                                  hjust = 0),\n        panel.spacing = unit(2, 'lines')) +\n  labs(title = 'Distribution of Fouls in Big 6 Games Both Home and Away',\n       caption = '(based on data from Evan Gower via Kaggle)',\n       y = 'Number of Fouls',\n       fill = 'In Big 6?') +\n  coord_flip() +\n  scale_fill_brewer(palette = 'Pastel2')\n\n\n\n\nThis plot shows the distribution of fouls for all the Big 6 teams both home and away as well as their opponents. When I had this idea, I was expecting to see a somewhat lower distribution for all of the Big 6 teams relative to their opponent, but I was surprised. For the most part, the fouls seam relatively even with the biggest advantage appearing to be Arsenal in home games. Below I created a table to show the average number of fouls per game for each Big 6 team.\n\n\nTable 1\n\ntb6 &lt;-\nfouls_df |&gt;\n  filter(Big_6 == 'Yes') |&gt;\n  group_by(HomeTeam) |&gt;\n  summarise('Fouls per Game for Big 6 Teams' = mean(Fouls))\n\ntb &lt;-\n  fouls_df |&gt;\n  filter(Big_6 == 'No') |&gt;\n  group_by(HomeTeam) |&gt;\n  summarise('Fouls per Game for Opponents' = mean(Fouls)) |&gt;\n  select(-HomeTeam)\n\ntb &lt;-\n  bind_cols(tb6, tb)\n\nkable(tb)\n\n\n\n\n\n\n\n\n\nHomeTeam\nFouls per Game for Big 6 Teams\nFouls per Game for Opponents\n\n\n\n\nArsenal\n9.285714\n10.321429\n\n\nChelsea\n11.214286\n10.750000\n\n\nLiverpool\n9.214286\n8.000000\n\n\nMan City\n8.321429\n8.607143\n\n\nMan United\n10.357143\n8.464286\n\n\nTottenham\n10.357143\n10.535714\n\n\n\n\n\nThis table shows that only a few Big 6 teams average less than their opponents, those being Arsenal, Man City, and Tottenham. I would say that for the most part, this shows that there isn’t bias towards the Big 6.\n\n\nConclusion\nUltimately, it seems as if there isn’t bias towards the Big 6 in terms of the number of fouls for them and for their opponents. As well, I was hoping to create some form of model to predict this, but it wasn’t a good possibility with this data set. Looking to the future, I likely would have added in another question and made a new visual as this one is super busy and not the best to interpret, but the process to get there took a while so I’ve decided to keep it. I might come back to this data in the future to try and answer a different question.\n\n\nConnections to Class\nI used many techniques we’ve learned in class in this post. Some being the dplyr tools such as filtering and selecting, binding and merging tables. I also used most of the ggplot techniques from faceting, flipping the coordinates, and adjusting the theme elements to make it look nicer."
  }
]